<!doctype html>
<html>
    <meta charset="utf-8">
    <title>Syllabus</title>
    <link href="styles.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Roboto:400,700,500' rel='stylesheet' type='text/css'>
</html>
<body>
    <div class='card'>
<h1 align="center"><a name="_Toc337586134" id="_Toc337586134">NEURAL  NETWORKS</a></h1>
<p align="center" class="Syllabus_Code"><strong>Subject  Code: 10CS756 / 10IS756</strong><br />
<h2 align="center"><a name="_Toc337586135" id="_Toc337586135">PART – A</a></h2>
<h3><a name="_Toc337586136" id="_Toc337586136">UNIT – 1: Introduction</a></h3>
<p>What is a Neural  Network?, Human Brain, Models of Neuron, NeuralNetworks viewed as directed  graphs, Feedback, Network Architectures,Knowledge representation, Artificial  Intelligence and Neural Networks.<strong>7 Hours</strong></p>
<h3><a name="_Toc337586137" id="_Toc337586137">UNIT – 2: Learning Processes – 1</a></h3>
<p>Introduction,  Error-correction learning, Memory-based learning, Hebbianlearning, Competitive  learning,Boltzamann learning, Credit Assignmentproblem, Learning with a  Teacher, Learning without a Teacher, Learningtasks, Memory, Adaptation.<strong>6  Hours</strong></p>
<h3><a name="_Toc337586138" id="_Toc337586138">UNIT – 3:Learning Processes – 2, Single Layer  Perceptrons</a></h3>
<p>Statistical  nature of thelearning process, Statistical learning theory, Approximately  correct model oflearning.<br />
  Single Layer  Perceptrons: Introduction, Adaptive filtering problem,Unconstrained  optimization techniques, Linear least-squares filters<strong>, </strong>Leastmeansquare  algorithm, Learning curves, Learning rate annealing techniques,Perceptron,  Perceptron convergence theorem, Relation between thePerceptron and Bayes  classifier for a Gaussian environment.<strong>7 Hours</strong></p>
<h3><a name="_Toc337586139" id="_Toc337586139">UNIT – 4: Multilayer Perceptrons – 1</a></h3>
<p>Introduction,  Some preliminaries, BackpropagationAlgorithm, Summary of back-propagation  algorithm, XORproblem, Heuristics for making the back-propagation algorithm  performbetter, Output representation and decision rule, Computer experiment,  Featuredetection, Back-propagation and differentiation.<strong>6 Hours</strong></p>
        
    </div>
    <div class='card'>
<h2 align="center"><a name="_Toc337586140" id="_Toc337586140">PART - B</a></h2>
<h3><a name="_Toc337586141" id="_Toc337586141">UNIT – 5: Multilayer Perceptrons – 2</a></h3>
<p>Hessian matrix,  Generalization, approximationof functions, Cross validation, Network pruning  techniques, virtues andlimitations of back- propagation learning, Accelerated  convergence of backpropagation learning, Supervised learning viewed as an  optimization problem,Convolution networks.<strong>7 Hours</strong></p>
<h3><a name="_Toc337586142" id="_Toc337586142">UNIT – 6: Radial-Basic Function Networks – 1</a></h3>
<p>Introduction,  Cover’s theorem on theseparability of patterns, Interpolation problem,  Supervised learning as an illposedHypersurface reconstruction problem,  Regularization theory,Regularization networks, Generalized radial-basis  function networks, XORproblem, Estimation of the regularization parameter.<strong>6  Hours</strong></p>
<h3><a name="_Toc337586143" id="_Toc337586143">UNIT – 7: Radial-Basic Function Networks – 2,  Optimization – 1</a></h3>
<p>Approximationproperties  of RBF networks, Comparison of RBF networks and multilayerPerceptrons, Kernel  regression and it’s relation to RBF networks, Learningstrategies, Computer  experiment.Optimization using Hopfield networks: Traveling salesperson  problem,Solving simultaneous linear equations, Allocating documents  tomultiprocessors.<strong>6 Hours</strong></p>
<h3><a name="_Toc337586144" id="_Toc337586144">UNIT – 8: Optimization Methods – 2</a></h3>
<p>Iterated  gradient descent, Simulated Annealing, Random Search, Evolutionarycomputation-  Evolutionary algorithms, Initialization, Termination criterion,Reproduction,  Operators, Replacement, Schema theorem.<strong>7 Hours</strong></p>
    
    </div>
    <div class='card'> 
<h4>Text Books:</h4>
<p>1. Simon Haykin:  Neural Networks - A Comprehensive Foundation,2nd Edition, Pearson Education,  1999.<br />
  (Chapters  1.1-1.8, 2.1-2.15, 3.1-3.10, 4.1-4.19, 5.1-5.14)<br />
  2. Kishan  Mehrotra, Chilkuri K. Mohan, Sanjay Ranka: ArtificialNeural Networks, Penram  International Publishing, 1997.(Chapters 7.1-7.5)</p>
<h4>Reference Books:</h4>
<p>1. B.Yegnanarayana:  Artificial Neural Networks, PHI, 2001.</p>
    
    </div>

</body>